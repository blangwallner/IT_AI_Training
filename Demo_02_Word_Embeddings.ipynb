{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Demo - Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip'\n",
    "filename = wget.download(url)\n",
    "with zipfile.ZipFile(filename,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the embedding and defining some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Iterable, List, Optional, Set, Tuple\n",
    "\n",
    "from we_load import load_words\n",
    "import math\n",
    "import we_vectors as v\n",
    "from we_vectors import Vector\n",
    "from we_word import Word\n",
    "\n",
    "# Timing info for most_similar (100k words):\n",
    "# Original version: 7.3s\n",
    "# Normalized vectors: 3.4s\n",
    "\n",
    "def most_similar(base_vector: Vector, words: List[Word]) -> List[Tuple[float, Word]]:\n",
    "    \"\"\"Finds n words with smallest cosine similarity to a given word\"\"\"\n",
    "    words_with_distance = [(v.cosine_similarity_normalized(base_vector, w.vector), w) for w in words]\n",
    "    # We want cosine similarity to be as large as possible (close to 1)\n",
    "    sorted_by_distance = sorted(words_with_distance, key=lambda t: t[0], reverse=True)\n",
    "    return sorted_by_distance\n",
    "\n",
    "def print_most_similar(words: List[Word], text: str) -> None:\n",
    "    base_word = find_word(text, words)\n",
    "    if not base_word:\n",
    "        print(f\"Uknown word: {text}\")\n",
    "        return\n",
    "    print(f\"Words related to {base_word.text}:\")\n",
    "    sorted_by_distance = [\n",
    "        word.text for (dist, word) in\n",
    "            most_similar(base_word.vector, words)\n",
    "            if word.text.lower() != base_word.text.lower()\n",
    "        ]\n",
    "    print(', '.join(sorted_by_distance[:10]))\n",
    "\n",
    "def read_word() -> str:\n",
    "    return input(\"Type a word: \")\n",
    "\n",
    "def find_word(text: str, words: List[Word]) -> Optional[Word]:\n",
    "    try:\n",
    "        return next(w for w in words if text == w.text)\n",
    "    except StopIteration:\n",
    "        return None\n",
    "\n",
    "def closest_analogies(\n",
    "    left2: str, left1: str, right2: str, words: List[Word]\n",
    ") -> List[Tuple[float, Word]]:\n",
    "    word_left1 = find_word(left1, words)\n",
    "    word_left2 = find_word(left2, words)\n",
    "    word_right2 = find_word(right2, words)\n",
    "    if (not word_left1) or (not word_left2) or (not word_right2):\n",
    "        return []\n",
    "    vector = v.add(\n",
    "        v.sub(word_left1.vector, word_left2.vector),\n",
    "        word_right2.vector)\n",
    "    closest = most_similar(vector, words)[:10]\n",
    "    def is_redundant(word: str) -> bool:\n",
    "        \"\"\"\n",
    "        Sometimes the two left vectors are so close the answer is e.g.\n",
    "        \"shirt-clothing is like phone-phones\". Skip 'phones' and get the next\n",
    "        suggestion, which might be more interesting.\n",
    "        \"\"\"\n",
    "        word_lower = word.lower()\n",
    "        return (\n",
    "            left1.lower() in word_lower or\n",
    "            left2.lower() in word_lower or\n",
    "            right2.lower() in word_lower)\n",
    "    closest_filtered = [(dist, w) for (dist, w) in closest if not is_redundant(w.text)]\n",
    "    return closest_filtered\n",
    "\n",
    "def print_analogy(left2: str, left1: str, right2: str, words: List[Word]) -> None:\n",
    "    analogies = closest_analogies(left2, left1, right2, words)\n",
    "    if (len(analogies) == 0):\n",
    "        print(f\"{left2}-{left1} is like {right2}-?\")\n",
    "    else:\n",
    "        (dist, w) = analogies[0]\n",
    "        #alternatives = ', '.join([f\"{w.text} ({dist})\" for (dist, w) in analogies])\n",
    "        print(f\"{left2}-{left1} is like {right2}-{w.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/wiki-news-300d-1M.vec...\n",
      "Loaded 999995 words.\n",
      "Removed stop words, 970448 remain.\n",
      "Removed duplicates, 921219 remain.\n"
     ]
    }
   ],
   "source": [
    "words = load_words('data/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and [-0.02372573377498372, 0.011258389593861702, ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the embedding: 300\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of the embedding: {:d}\".format(len(words[0].vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words related to section:\n",
      "subsection, sections, paragraph, secion, secton, subheading, subsections, sction, setion, seciton\n",
      "Words related to question:\n",
      "answer, quesiton, questions, quesion, queston, qustion, ask, whether, answers, non-question\n",
      "Words related to development:\n",
      "growth, developments, implementation, developement, construction, developing, advancement, research, developmental, design\n",
      "Words related to staff:\n",
      "staffs, personnel, staffers, faculty, members, employees, assistants, staffer, officers, consultants\n"
     ]
    }
   ],
   "source": [
    "print_most_similar(words, words[190].text)\n",
    "print_most_similar(words, words[230].text)\n",
    "print_most_similar(words, words[330].text)\n",
    "print_most_similar(words, words[430].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a word: Austria\n",
      "Words related to Austria:\n",
      "Vienna, Austrian, Germany, Salzburg, Graz, Hungary, Innsbruck, Switzerland, Styria, Bavaria\n"
     ]
    }
   ],
   "source": [
    "text = read_word()\n",
    "w = find_word(text, words)\n",
    "if not w:\n",
    "    print(\"Sorry, I don't know that word.\")\n",
    "else:\n",
    "    print_most_similar(words, w.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Looking at analogies -- or computing with words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man-him is like woman-her\n",
      "quick-quickest is like far-furthest\n",
      "sushi-rice is like pizza-wheat\n",
      "Paris-France is like Rome-Italy\n",
      "dog-mammal is like eagle-bird\n",
      "German-BMW is like American-Lexus\n",
      "German-Opel is like American-Chrysler\n"
     ]
    }
   ],
   "source": [
    "print_analogy('man', 'him' , 'woman', words)\n",
    "# You'll need to download the pretrained word vectors to complete the analogies\n",
    "# below:\n",
    "# https://fasttext.cc/docs/en/english-vectors.html\n",
    "print_analogy('quick', 'quickest' , 'far', words)\n",
    "print_analogy('sushi', 'rice', 'pizza', words)\n",
    "print_analogy('Paris', 'France', 'Rome', words)\n",
    "print_analogy('dog', 'mammal', 'eagle', words)\n",
    "print_analogy('German', 'BMW' , 'American', words)\n",
    "print_analogy('German', 'Opel', 'American', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a word: a\n",
      "Sorry, I don't know that word.\n",
      "Type a word: q\n",
      "Sorry, I don't know that word.\n",
      "Type a word: \n",
      "Sorry, I don't know that word.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b4ae02ad1540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mright2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sorry, I don't know that word.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint_analogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "left2 = find_word(read_word(), words)\n",
    "if not left2:\n",
    "    print(\"Sorry, I don't know that word.\")\n",
    "left1 = find_word(read_word(), words)\n",
    "if not left1:\n",
    "    print(\"Sorry, I don't know that word.\")\n",
    "right2 = find_word(read_word(), words)\n",
    "if not right2:\n",
    "    print(\"Sorry, I don't know that word.\")\n",
    "print_analogy(left2.text, left1.text, right2.text, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "it_ai_training",
   "language": "python",
   "name": "it_ai_training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
