{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Natural Language Processing Case Study\n",
    "\n",
    "## Sentiment Classification for Customer reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wget\n",
      "  Using cached wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=11569 sha256=6ae1cfe4335f14ead48dc2e0e60f44456d8119dacb1807a554bbaaec8ef9f31f\n",
      "  Stored in directory: /home/blangwallner/.cache/pip/wheels/90/1d/93/c863ee832230df5cfc25ca497b3e88e0ee3ea9e44adc46ac62\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "url = 'https://www.dropbox.com/s/bzvfxltd1mokp9l/amazon_reviews_train_small.csv?dl=1'\n",
    "filename = wget.download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# import sklearn classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('amazon_reviews_train_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305410</th>\n",
       "      <td>1</td>\n",
       "      <td>Doesn't Like to Share</td>\n",
       "      <td>We purchased this item thinking it would be ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400963</th>\n",
       "      <td>2</td>\n",
       "      <td>AN OKAY BUT NOT GREAT ACTION-DISASTER FILCK</td>\n",
       "      <td>VOLCANO with TOMMY LEE JONES AND ANNE HECHE is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372049</th>\n",
       "      <td>4</td>\n",
       "      <td>Good wheel...</td>\n",
       "      <td>Our (light weight) teddy bear hamster has used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205099</th>\n",
       "      <td>1</td>\n",
       "      <td>InStilling Fear is more like it</td>\n",
       "      <td>I hated this book. It may be well written but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399411</th>\n",
       "      <td>3</td>\n",
       "      <td>Not as good as 2.0</td>\n",
       "      <td>Both of the first two CD's are in my car right...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        grade                                        title  \\\n",
       "305410      1                        Doesn't Like to Share   \n",
       "400963      2  AN OKAY BUT NOT GREAT ACTION-DISASTER FILCK   \n",
       "372049      4                                Good wheel...   \n",
       "205099      1              InStilling Fear is more like it   \n",
       "399411      3                           Not as good as 2.0   \n",
       "\n",
       "                                                     text  \n",
       "305410  We purchased this item thinking it would be ea...  \n",
       "400963  VOLCANO with TOMMY LEE JONES AND ANNE HECHE is...  \n",
       "372049  Our (light weight) teddy bear hamster has used...  \n",
       "205099  I hated this book. It may be well written but ...  \n",
       "399411  Both of the first two CD's are in my car right...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade =\n",
      "2\n",
      "\n",
      "Text = Good while it lasted\n"
     ]
    }
   ],
   "source": [
    "i = 8\n",
    "print('Grade =\\n{}\\n'.format(data.iloc[i,0]))\n",
    "print(\"Text = {}\".format(data.iloc[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Analyze Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check distribution of grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade\n",
       "1        20199\n",
       "3        20049\n",
       "4        19922\n",
       "5        19919\n",
       "2        19911\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['grade']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nicely balanced dataset!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "\n",
    "### <span style=\"color:blue\">**TODO: Experiment with vectorizers**</span>\n",
    "\n",
    "* Change parameters of `CountVectorizer` and observe performance, training time etc.\n",
    "* Try `TfidfVectorizer`, change parameters and observe performance, training time etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set stopwords\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "# initialize and fit vectorizer\n",
    "vect = CountVectorizer(max_features=3000, stop_words=stopwords, token_pattern=r'\\b[^\\d\\W]+\\b')\\\n",
    "                      .fit(data['text'])\n",
    "\n",
    "# apply vectorizer to data set\n",
    "X = vect.transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of discarded tokens: since we chose to use only the most frequent tokens, the number of stop words goes up a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104803\n"
     ]
    }
   ],
   "source": [
    "print(len(vect.stop_words_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['grade'].apply(lambda x: 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305410    0\n",
       "400963    0\n",
       "372049    1\n",
       "205099    0\n",
       "399411    0\n",
       "         ..\n",
       "273545    1\n",
       "168030    0\n",
       "22598     0\n",
       "68302     1\n",
       "2178      0\n",
       "Name: grade, Length: 100000, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic information on tokens and size of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3000\n",
      "\n",
      "First 100 features:\n",
      "['ability', 'able', 'absolute', 'absolutely', 'abuse', 'ac', 'accept', 'acceptable', 'access', 'accident', 'according', 'account', 'accuracy', 'accurate', 'acoustic', 'act', 'acted', 'acting', 'action', 'actions', 'active', 'actor', 'actors', 'actress', 'acts', 'actual', 'actually', 'ad', 'adapter', 'add', 'added', 'adding', 'addition', 'additional', 'address', 'adds', 'adequate', 'adjust', 'adjustable', 'admit', 'adult', 'adults', 'advance', 'advanced', 'advantage', 'adventure', 'adventures', 'advertised', 'advertising', 'advice', 'advise', 'afraid', 'african', 'afternoon', 'age', 'ages', 'ago', 'agree', 'ahead', 'aid', 'ain', 'air', 'al', 'alarm', 'album', 'albums', 'alice', 'alien', 'alive', 'allow', 'allowed', 'allows', 'alot', 'alright', 'alternative', 'amazed', 'amazing', 'amazon', 'america', 'american', 'americans', 'amusing', 'analysis', 'ancient', 'angel', 'angle', 'angry', 'animal', 'animals', 'animation', 'anime', 'anne', 'annoyed', 'annoying', 'answer', 'answers', 'antenna', 'anti', 'anybody', 'anymore']\n",
      "\n",
      "Features 110 to 130:\n",
      "['application', 'applied', 'apply', 'appreciate', 'appreciated', 'approach', 'appropriate', 'area', 'areas', 'aren', 'arm', 'arms', 'army', 'arrangements', 'arrive', 'arrived', 'art', 'articles', 'artist', 'artists']\n",
      "\n",
      "Every 100th feature:\n",
      "['ability', 'apart', 'battles', 'broke', 'chapter', 'complaints', 'cultural', 'difficult', 'easy', 'explain', 'fit', 'girlfriend', 'healthy', 'included', 'keeps', 'lightweight', 'marketing', 'mount', 'opera', 'physical', 'printing', 'realized', 'rid', 'seeing', 'skin', 'stayed', 'swing', 'title', 'updated', 'weeks']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "\n",
    "print(\"Number of features: {}\\n\".format(len(feature_names)))\n",
    "print(\"First 100 features:\\n{}\\n\".format(feature_names[:100]))\n",
    "print(\"Features 110 to 130:\\n{}\\n\".format(feature_names[110:130]))\n",
    "print(\"Every 100th feature:\\n{}\\n\".format(feature_names[::100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choose classifier method and fit on data\n",
    "_______________\n",
    "### <span style=\"color:blue\">**TODO: Experiment with different classifier choices and different parameters!**</span>\n",
    "\n",
    "Use e.g. `DecisionTreeClassifier`, `RandomForestClassifier`, `GaussianNB`, `MLPClassifier` with useful hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = clf.fit(X_train, y_train)\n",
    "clf = clf.fit(X_train.toarray(), np.array(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use trained model to predict labels for train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test.toarray())\n",
    "y_train_pred = clf.predict(X_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set = 81.1%\n",
      "accuracy on test set\t = 78.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy on training set = {:1.1f}%\".format(100*train_accuracy))\n",
    "print(\"accuracy on test set\\t = {:1.1f}%\".format(100*test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10039  1988]\n",
      " [ 2274  5699]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.82     12027\n",
      "           1       0.74      0.71      0.73      7973\n",
      "\n",
      "    accuracy                           0.79     20000\n",
      "   macro avg       0.78      0.77      0.78     20000\n",
      "weighted avg       0.79      0.79      0.79     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['comes', 'cover', 'description', 'disappointed', 'does', 'hard',\n",
       "        'kitchen', 'notice', 'page', 'print', 'read', 's', 'say', 'small',\n",
       "        'white'], dtype='<U14'),\n",
       " array(['bottle', 'did', 'filler', 'fit', 'great', 'help', 'insert',\n",
       "        'just', 'kit', 'lamp', 'manage', 'opening', 'perfectly', 'quality',\n",
       "        'quite', 'small', 'use', 'using', 'wanted', 'wine', 'wondering'],\n",
       "       dtype='<U14'),\n",
       " array(['afraid', 'auto', 'b', 'buy', 'concept', 'd', 'daily', 'did',\n",
       "        'duty', 'excellent', 'expect', 'expected', 'great', 'highest',\n",
       "        'house', 'light', 'odd', 'product', 'quality', 'room', 'seriously',\n",
       "        'stuck', 'tool', 'use'], dtype='<U14'),\n",
       " array(['beat', 'bible', 'book', 'community', 'fish', 'good', 'isn',\n",
       "        'just', 'kinda', 'know', 'like', 'ones', 'really', 'shopping',\n",
       "        'similar', 't', 'tank', 'want'], dtype='<U14'),\n",
       " array(['book', 'common', 'computer', 'connect', 'details', 'equipment',\n",
       "        'essential', 'experience', 'features', 'function', 'gives', 'goes',\n",
       "        'hardware', 'home', 'information', 'know', 'learn', 'majority',\n",
       "        'music', 'need', 'outdated', 'playing', 'probably', 'record',\n",
       "        'recording', 'regarding', 'setting', 'software', 'start',\n",
       "        'started', 'studio', 'technical', 'updated', 'use', 'works'],\n",
       "       dtype='<U14')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.inverse_transform(X_test.toarray()[y_test!=y_pred])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Look at tokens associated with positive or negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words associated with negative reviews:\n",
      "['poorly' 'waste' 'disappointing' 'disappointment' 'mediocre' 'worst'\n",
      " 'worthless' 'awful' 'terrible' 'useless' 'returning' 'refund' 'junk'\n",
      " 'returned' 'shallow' 'misleading' 'cheaply' 'tedious' 'horrible' 'fails'\n",
      " 'lacks' 'bland' 'overpriced' 'boring' 'lacked' 'disappointed' 'poor'\n",
      " 'worse' 'frustrating' 'contrived' 'lame' 'hopes' 'unfortunately'\n",
      " 'unrealistic' 'ripped' 'outdated' 'okay' 'pointless' 'garbage' 'sucks'\n",
      " 'lacking' 'predictable' 'dull' 'defective' 'premise' 'loses' 'barely'\n",
      " 'ruined' 'disapointed' 'irritating' 'threw' 'ok' 'beware' 'wasted'\n",
      " 'terribly' 'hoping' 'ridiculous' 'sorry' 'distracting' 'sadly' 'stopped'\n",
      " 'dissapointed' 'weak' 'return' 'cracked' 'trash' 'failed' 'generic'\n",
      " 'falls' 'repetitive' 'flimsy' 'joke' 'unless' 'needless' 'hype' 'warned'\n",
      " 'pathetic' 'depressing' 'halfway' 'lousy' 'substance' 'static' 'broke'\n",
      " 'rushed' 'simplistic' 'frustrated' 'suppose' 'bothered' 'dumb' 'sentence'\n",
      " 'crap' 'alright' 'rip' 'whatsoever' 'uncomfortable' 'bored' 'supposedly'\n",
      " 'hoped' 'wasting' 'zero']\n",
      "\n",
      "\n",
      "Words associated with positive reviews:\n",
      "['brilliant' 'beautiful' 'winner' 'faster' 'comfortable' 'best' 'keeps'\n",
      " 'adjustable' 'powerful' 'thanks' 'gives' 'ease' 'love' 'enjoying'\n",
      " 'surprised' 'exactly' 'captured' 'improved' 'unlike' 'smile' 'genius'\n",
      " 'pleasure' 'intense' 'explains' 'packed' 'treasure' 'valuable'\n",
      " 'definately' 'loved' 'happy' 'stunning' 'fascinating' 'lifetime' 'kinds'\n",
      " 'favorite' 'easy' 'accident' 'stores' 'glad' 'sturdy' 'thoroughly'\n",
      " 'handy' 'fabulous' 'laughed' 'vivid' 'great' 'helped' 'thank' 'promptly'\n",
      " 'concise' 'allows' 'amazed' 'worry' 'brings' 'favorites' 'notch'\n",
      " 'delivers' 'ages' 'twists' 'surprisingly' 'minor' 'satisfied' 'drawback'\n",
      " 'favourite' 'rocks' 'crisp' 'hilarious' 'fantastic' 'laughing' 'helps'\n",
      " 'outstanding' 'penny' 'enjoys' 'complaints' 'timely' 'wonderful'\n",
      " 'wonderfully' 'hooked' 'complaint' 'exceptional' 'beautifully'\n",
      " 'incredible' 'perfectly' 'loves' 'worried' 'terrific' 'perfect' 'finest'\n",
      " 'amazing' 'delightful' 'provoking' 'disappoint' 'captures' 'excellent'\n",
      " 'highly' 'superb' 'gem' 'awesome' 'refreshing']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    indices = np.argsort(clf.coef_[0])\n",
    "    feature_names = np.array(vect.get_feature_names())[indices]\n",
    "    neg_unigrams = feature_names[:100]\n",
    "    print('Words associated with negative reviews:')\n",
    "    print(neg_unigrams)\n",
    "    pos_unigrams = feature_names[-100:-1]\n",
    "    print('\\n')\n",
    "    print('Words associated with positive reviews:')\n",
    "    print(pos_unigrams)\n",
    "except:\n",
    "    print('Print classifier does not have coeff attribute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply sentiment classification to new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = lambda x: 'positive' if x == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">**TODO: Write your own review and check the detected sentiment!**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_review = ['this product is really bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment of review: negative\n"
     ]
    }
   ],
   "source": [
    "# vectorize new review\n",
    "x_rev = vect.transform(my_review)\n",
    "# Classify new review\n",
    "y_pred = clf.predict(x_rev.toarray())\n",
    "print('Sentiment of review: {}'.format(sentiment(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model to pickle files for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open('nlp_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vect, open('nlp_vect.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "it_ai_training",
   "language": "python",
   "name": "it_ai_training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
